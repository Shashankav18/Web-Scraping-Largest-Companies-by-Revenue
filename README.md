# ğŸ¢ Web Scraping Project: Largest Companies by Revenue

This project extracts the **list of the world's largest companies by revenue** from a public website (Wikipedia) using **Python, BeautifulSoup, and Pandas**.  
It demonstrates how to automate data extraction, cleaning, and storage using web scraping techniques.

---

## ğŸ“Š Project Overview

The goal of this project is to scrape company data such as:
- **Rank**
- **Company Name**
- **Industry**
- **Revenue (in USD Billions)**
- **Headquarters**

The extracted data is then cleaned and saved as a CSV file for further analysis.

---

## âš™ï¸ Tools & Libraries Used

| Tool | Purpose |
|------|----------|
| **Python 3** | Programming language used |
| **Requests** | To send HTTP requests to fetch HTML content |
| **BeautifulSoup (bs4)** | To parse and extract data from HTML |
| **Pandas** | To structure, clean, and export data into CSV |
| **Jupyter Notebook / VS Code** | For development and testing |

---

## ğŸ“‚ Folder Structure

```
/largest-companies-by-revenue-scraper
â”‚
â”œâ”€â”€ main.py                         # Python script for scraping and saving data
â”œâ”€â”€ requirements.txt                 # Dependencies used
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ largest_companies.csv        # Final scraped dataset
â”‚
â”‚
â”œâ”€â”€ README.md                        # Documentation
â””â”€â”€ LICENSE                          # Optional
```

---

## ğŸš€ How to Run the Project

1. **Clone this repository**
   ```bash
   git clone https://github.com/yourusername/largest-companies-by-revenue-scraper.git
   cd largest-companies-by-revenue-scraper
   ```

2. **Install required libraries**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the scraper**
   ```bash
   python main.py
   ```

4. **View the output**
   - The data will be saved in the folder:  
     `data/largest_companies.csv`

---

## ğŸ–¼ï¸ Example Output

![Scraped Output Preview](screenshots/scraped_output_preview.png)

| Rank | Company | Industry | Revenue (USD billions) | Headquarters |
|------|----------|-----------|------------------------|---------------|
| 1 | Walmart | Retail | 611.3 | United States |
| 2 | Saudi Aramco | Oil & Gas | 603.7 | Saudi Arabia |
| 3 | Amazon | E-commerce | 513.9 | United States |
| ... | ... | ... | ... | ... |

---

## ğŸ’¡ Key Learning Points

- Using `requests` to fetch website HTML content  
- Parsing and extracting HTML table data using `BeautifulSoup`  
- Cleaning and structuring data with `pandas`  
- Exporting data into `.csv` format  
- Following ethical web scraping guidelines (avoiding excessive requests, crediting source)

---

## âš ï¸ Ethical Note

This project is for **educational purposes only**.  
Always review a websiteâ€™s **robots.txt** and **terms of service** before scraping.

---

## ğŸ§‘â€ğŸ’» Author

**Shashank AV**

---

## ğŸ“œ License

This project is open-source and available under the **MIT License**.  
Feel free to use, modify, and share with proper credit.

---

## ğŸ§° requirements.txt

```text
requests
beautifulsoup4
pandas
```
